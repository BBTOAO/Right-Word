{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a95eec0-4b0c-4892-9682-338ae684e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reddit Emotional Narrative Analysis & Advanced Linguistic Processing\n",
    "# \n",
    "# This notebook implements a system to identify, extract, and analyse emotionally rich narratives from Reddit for targeted marketing.\n",
    "#\n",
    "# **Processing Steps Overview:**\n",
    "# 1. **Setup & Subreddit Identification:**  \n",
    "#    - Define inputs for the target market and product/service.\n",
    "#\n",
    "# 2. **Reddit Data Retrieval:**  \n",
    "#    - Retrieve posts and comments using PRAW and the Pushshift API.\n",
    "#\n",
    "# 3. **Emotional Story Extraction & Advanced Language Processing:**  \n",
    "#    - Extract narratives using sentiment analysis (VADER) and text filtering.\n",
    "#    - **Advanced Linguistic Analysis:**  \n",
    "#      Analyse each narrative for a wide range of linguistic features, including:\n",
    "#         - **Discourse Analysis:** Detect discourse markers.\n",
    "#         - **Pragmatics:** Identify speech acts.\n",
    "#         - **Metaphor/Simile Detection:** Flag figurative language.\n",
    "#         - **Semantic Role Labeling:** (Simulated output)\n",
    "#         - **Narrative Transportation:** (Readability as a proxy)\n",
    "#         - **Affect Theory Integration:** Map sentiment to discrete emotions (simplified Plutchik’s wheel)\n",
    "#         - **Narrative Structure Segmentation:** Divide text into orientation, complicating action, and resolution.\n",
    "#         - **Expanded Emotional Frameworks:** Detect evaluative language.\n",
    "#         - **Entity Tracking & Coreference:** (Simulated output)\n",
    "#         - **Sarcasm & Irony Detection:** (Simple heuristic)\n",
    "#         - **Stance Detection:** Based on sentiment polarity.\n",
    "#\n",
    "# 4. **Detailed Data Analysis & Visualisation:**  \n",
    "#    - Visualise narrative distributions, sentiment scores, engagement metrics, and emotional arcs.\n",
    "#\n",
    "# 5. **Advanced Narrative Processing, Influencer Mapping & Interactive Dashboard:**  \n",
    "#    - Cluster narratives, extract key entities, build an entity co-occurrence network, and offer an interactive dashboard.\n",
    "#\n",
    "# **Note:**  \n",
    "# Some advanced linguistic techniques are implemented in a simplified manner for demonstration.\n",
    "\n",
    "# =====================\n",
    "# Section 1: Setup & Subreddit Identification\n",
    "# =====================\n",
    "\n",
    "import logging\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define input parameters \n",
    "target_market = \"prospective university students\"  \n",
    "product_service = \"a university’s new programme in data science\"\n",
    "\n",
    "logging.info(\"Defined inputs: target_market='%s', product_service='%s'\", target_market, product_service)\n",
    "\n",
    "def generate_subreddits(target_market: str, product_service: str) -> list:\n",
    "    \"\"\"\n",
    "    Simulate subreddit suggestions based on input parameters.\n",
    "    \"\"\"\n",
    "    suggested_subreddits = []\n",
    "    \n",
    "    # If the product/service is related to a university, suggest university-related subreddits.\n",
    "    if \"university\" in product_service.lower():\n",
    "        suggested_subreddits.extend([\n",
    "            \"r/University\",\n",
    "            \"r/UniversityAdmissions\",\n",
    "            \"r/StudentLife\",\n",
    "            \"r/UniExperience\"\n",
    "        ])\n",
    "    # If the product/service is related to fitness, use fitness-related subreddits.\n",
    "    elif \"fitness\" in product_service.lower() or \"workout\" in product_service.lower():\n",
    "        suggested_subreddits.extend([\n",
    "            \"r/fitness\",\n",
    "            \"r/bodybuilding\",\n",
    "            \"r/running\",\n",
    "            \"r/HealthyFood\"\n",
    "        ])\n",
    "    # Default fallback subreddits.\n",
    "    else:\n",
    "        suggested_subreddits.extend([\n",
    "            \"r/AskReddit\",\n",
    "            \"r/TrueOffMyChest\",\n",
    "            \"r/relationships\",\n",
    "            \"r/lifehacks\"\n",
    "        ])\n",
    "    \n",
    "    # Further adjust suggestions based on the target market.\n",
    "    if \"prospective university\" in target_market.lower() and \"r/University\" not in suggested_subreddits:\n",
    "        suggested_subreddits.append(\"r/University\")\n",
    "        \n",
    "    logging.info(\"Subreddit suggestions: %s\", suggested_subreddits)\n",
    "    return suggested_subreddits\n",
    "\n",
    "# Generate subreddit suggestions\n",
    "subreddit_list = generate_subreddits(target_market, product_service)\n",
    "print(\"Suggested Subreddits:\")\n",
    "for sub in subreddit_list:\n",
    "    print(f\"- {sub}\")\n",
    "\n",
    "# # Section 2: Reddit Data Retrieval\n",
    "#\n",
    "# In this section, we retrieve posts using:\n",
    "# - **PRAW:** For real-time posts and comments\n",
    "# - **Pushshift API:** For complementary historical data.\n",
    "\n",
    "import praw\n",
    "\n",
    "REDDIT_CLIENT_ID = \"YOUR_CLIENT_ID\"\n",
    "REDDIT_CLIENT_SECRET = \"YOUR_CLIENT_SECRET\"\n",
    "REDDIT_USER_AGENT = \"NarrativeExtractionBot/0.1 by YourUsername\"\n",
    "\n",
    "# Initialise the PRAW Reddit instance.\n",
    "reddit = praw.Reddit(client_id=REDDIT_CLIENT_ID,\n",
    "                     client_secret=REDDIT_CLIENT_SECRET,\n",
    "                     user_agent=REDDIT_USER_AGENT)\n",
    "logging.info(\"Initialised Reddit instance via PRAW.\")\n",
    "\n",
    "def fetch_reddit_posts(subreddit_name: str, limit: int = 50) -> list:\n",
    "    \"\"\"\n",
    "    Retrieve posts and a sample of comments from a subreddit using PRAW.\n",
    "    \"\"\"\n",
    "    posts_data = []\n",
    "    try:\n",
    "        clean_subreddit = subreddit_name.replace(\"r/\", \"\")\n",
    "        subreddit = reddit.subreddit(clean_subreddit)\n",
    "        for submission in subreddit.hot(limit=limit):\n",
    "            post_info = {\n",
    "                \"id\": submission.id,\n",
    "                \"title\": submission.title,\n",
    "                \"selftext\": submission.selftext,\n",
    "                \"score\": submission.score,\n",
    "                \"created_utc\": submission.created_utc,\n",
    "                \"num_comments\": submission.num_comments,\n",
    "                \"url\": submission.url,\n",
    "                \"subreddit\": subreddit_name,\n",
    "                \"comments\": []\n",
    "            }\n",
    "            submission.comments.replace_more(limit=0)\n",
    "            for comment in submission.comments.list()[:10]:\n",
    "                comment_data = {\n",
    "                    \"id\": comment.id,\n",
    "                    \"body\": comment.body,\n",
    "                    \"score\": comment.score,\n",
    "                    \"created_utc\": comment.created_utc\n",
    "                }\n",
    "                post_info[\"comments\"].append(comment_data)\n",
    "            posts_data.append(post_info)\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error fetching posts from %s: %s\", subreddit_name, e)\n",
    "    logging.info(\"Fetched %d posts from %s via PRAW.\", len(posts_data), subreddit_name)\n",
    "    return posts_data\n",
    "\n",
    "def fetch_pushshift_posts(subreddit_name: str, limit: int = 50) -> list:\n",
    "    \"\"\"\n",
    "    Retrieve posts from a subreddit using the Pushshift API.\n",
    "    \"\"\"\n",
    "    clean_subreddit = subreddit_name.replace(\"r/\", \"\")\n",
    "    url = f\"https://api.pushshift.io/reddit/submission/search/?subreddit={clean_subreddit}&size={limit}\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            posts = data.get(\"data\", [])\n",
    "            logging.info(\"Fetched %d posts from %s via Pushshift.\", len(posts), subreddit_name)\n",
    "            return posts\n",
    "        else:\n",
    "            logging.error(\"Pushshift API error for %s: HTTP %s\", subreddit_name, response.status_code)\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error fetching Pushshift posts from %s: %s\", subreddit_name, e)\n",
    "        return []\n",
    "\n",
    "# Aggregate data from all suggested subreddits\n",
    "MAX_POSTS_PER_SUBREDDIT = 50\n",
    "reddit_data = {}\n",
    "\n",
    "for sub in subreddit_list:\n",
    "    logging.info(\"Retrieving data for %s\", sub)\n",
    "    praw_posts = fetch_reddit_posts(sub, limit=MAX_POSTS_PER_SUBREDDIT)\n",
    "    pushshift_posts = fetch_pushshift_posts(sub, limit=MAX_POSTS_PER_SUBREDDIT)\n",
    "    reddit_data[sub] = {\n",
    "        \"praw_posts\": praw_posts,\n",
    "        \"pushshift_posts\": pushshift_posts\n",
    "    }\n",
    "\n",
    "print(\"\\nData Retrieval Summary:\")\n",
    "for sub, data in reddit_data.items():\n",
    "    print(f\"- {sub}: {len(data['praw_posts'])} posts (PRAW), {len(data['pushshift_posts'])} posts (Pushshift)\")\n",
    "\n",
    "# Emotional Story Extraction & Advanced Language Processing\n",
    "#\n",
    "# Here we extract emotionally compelling narratives and further analyse them using a wide range of linguistic techniques.\n",
    "#\n",
    "# We use:\n",
    "# - **spaCy:** For language processing.\n",
    "# - **NLTK’s VADER:** For sentiment analysis.\n",
    "#\n",
    "# Additionally, we perform an advanced linguistic analysis on each narrative covering:\n",
    "#   - Discourse Analysis, Pragmatics, Metaphor/Simile Detection, Semantic Role Labeling,\n",
    "#     Narrative Transportation, Affect Theory Integration, Narrative Structure Segmentation,\n",
    "#     Expanded Emotional Frameworks, Entity Tracking & Coreference, Sarcasm & Irony Detection,\n",
    "#     and Stance Detection.\n",
    "\n",
    "import spacy\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute and return the VADER compound sentiment score for the given text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    clean_text = \" \".join(token.text for token in doc)\n",
    "    sentiment = sentiment_analyzer.polarity_scores(clean_text)\n",
    "    return sentiment[\"compound\"]\n",
    "\n",
    "def advanced_linguistic_analysis(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Perform an advanced linguistic analysis on the input text covering multiple aspects:\n",
    "      1. Discourse Analysis: Detect discourse markers.\n",
    "      2. Pragmatics: Identify speech acts (e.g., questions, exclamations).\n",
    "      3. Metaphor/Simile Detection: Search for figurative language.\n",
    "      4. Semantic Role Labeling: (Simulated output)\n",
    "      5. Narrative Transportation: Use readability (Flesch Reading Ease) as a proxy.\n",
    "      6. Affect Theory Integration: Map sentiment to simplified emotion categories.\n",
    "      7. Narrative Structure Segmentation: Divide text into orientation, complicating action, and resolution.\n",
    "      8. Expanded Emotional Frameworks: Detect evaluative language.\n",
    "      9. Entity Tracking & Coreference: (Simulated output)\n",
    "     10. Sarcasm & Irony Detection: Simple heuristic-based check.\n",
    "     11. Stance Detection: Based on sentiment polarity.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # 1. Discourse Analysis: Detect common discourse markers.\n",
    "    discourse_markers_list = [\"however\", \"moreover\", \"furthermore\", \"but\", \"so\", \"therefore\", \"thus\", \"meanwhile\", \"in contrast\"]\n",
    "    discourse_markers_found = [marker for marker in discourse_markers_list if marker in text.lower()]\n",
    "    result[\"discourse_markers\"] = discourse_markers_found\n",
    "    \n",
    "    # 2. Pragmatics: Count interrogative and exclamatory sentences.\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    question_sentences = [s for s in sentences if s.strip().endswith(\"?\")]\n",
    "    exclamatory_sentences = [s for s in sentences if s.strip().endswith(\"!\")]\n",
    "    result[\"num_questions\"] = len(question_sentences)\n",
    "    result[\"num_exclamations\"] = len(exclamatory_sentences)\n",
    "    \n",
    "    # 3. Metaphor/Simile Detection: Simple heuristic for \"as ... as\" and \"like a\" patterns.\n",
    "    simile_matches = re.findall(r'\\bas\\s+\\w+\\s+as\\b', text, flags=re.IGNORECASE)\n",
    "    metaphor_matches = re.findall(r'\\blike a\\b', text, flags=re.IGNORECASE)\n",
    "    result[\"similes\"] = simile_matches\n",
    "    result[\"metaphors\"] = metaphor_matches\n",
    "    \n",
    "    # 4. Semantic Role Labeling: Placeholder (simulate extraction of Agent, Action, Object).\n",
    "    result[\"semantic_roles\"] = \"Simulated SRL: [Agent, Action, Object] structure extracted.\"\n",
    "    \n",
    "    # 5. Narrative Transportation: Use readability (Flesch Reading Ease) as a proxy.\n",
    "    try:\n",
    "        import textstat\n",
    "        readability = textstat.flesch_reading_ease(text)\n",
    "    except ImportError:\n",
    "        readability = 50.0  # Default value if textstat is not installed.\n",
    "    result[\"transportation_score\"] = readability\n",
    "    \n",
    "    # 6. Affect Theory Integration: Map sentiment to discrete emotions.\n",
    "    compound = get_sentiment_score(text)\n",
    "    if compound >= 0.5:\n",
    "        result[\"affect_emotions\"] = \"Joy/Anticipation\"\n",
    "    elif compound <= -0.5:\n",
    "        result[\"affect_emotions\"] = \"Sadness/Anger\"\n",
    "    else:\n",
    "        result[\"affect_emotions\"] = \"Neutral\"\n",
    "    \n",
    "    # 7. Narrative Structure Segmentation: Divide text into three parts (if possible).\n",
    "    if len(sentences) >= 3:\n",
    "        third = len(sentences) // 3\n",
    "        orientation = \" \".join(sentences[:third])\n",
    "        complicating_action = \" \".join(sentences[third:2*third])\n",
    "        resolution = \" \".join(sentences[2*third:])\n",
    "    else:\n",
    "        orientation = text\n",
    "        complicating_action = \"\"\n",
    "        resolution = \"\"\n",
    "    result[\"narrative_structure\"] = {\n",
    "        \"orientation\": orientation,\n",
    "        \"complicating_action\": complicating_action,\n",
    "        \"resolution\": resolution\n",
    "    }\n",
    "    \n",
    "    # 8. Expanded Emotional Frameworks: Detect evaluative adjectives.\n",
    "    evaluative_terms = [\"good\", \"bad\", \"excellent\", \"terrible\", \"amazing\", \"awful\", \"incredible\", \"horrible\", \"wonderful\", \"disappointing\"]\n",
    "    found_evaluative = [word for word in evaluative_terms if word in text.lower()]\n",
    "    result[\"evaluative_terms\"] = found_evaluative\n",
    "    \n",
    "    # 9. Entity Tracking & Coreference: Simulated output.\n",
    "    result[\"coreference\"] = \"Simulated coreference chains: [Entity1 -> pronoun, Entity2 -> pronoun].\"\n",
    "    \n",
    "    # 10. Sarcasm & Irony Detection: Simple heuristic based on key phrases.\n",
    "    if \"yeah, right\" in text.lower() or \"as if\" in text.lower():\n",
    "        result[\"sarcasm_detected\"] = True\n",
    "    else:\n",
    "        result[\"sarcasm_detected\"] = False\n",
    "    \n",
    "    # 11. Stance Detection: Use sentiment polarity as a proxy.\n",
    "    if compound > 0.2:\n",
    "        result[\"stance\"] = \"Support\"\n",
    "    elif compound < -0.2:\n",
    "        result[\"stance\"] = \"Oppose\"\n",
    "    else:\n",
    "        result[\"stance\"] = \"Neutral\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "def extract_emotional_narratives(reddit_data: dict, sentiment_threshold: float = 0.5, length_threshold: int = 50) -> list:\n",
    "    \"\"\"\n",
    "    Iterate through Reddit posts and extract those with strong emotional signals.\n",
    "    Additionally, perform advanced linguistic analysis on each narrative.\n",
    "    \"\"\"\n",
    "    emotional_narratives = []\n",
    "    for subreddit, data in reddit_data.items():\n",
    "        for post in data.get(\"praw_posts\", []):\n",
    "            # Combine title and selftext to form the narrative.\n",
    "            narrative_text = f\"{post.get('title', '')}\\n{post.get('selftext', '')}\"\n",
    "            if len(narrative_text) < length_threshold:\n",
    "                continue\n",
    "            sentiment_score = get_sentiment_score(narrative_text)\n",
    "            if abs(sentiment_score) >= sentiment_threshold:\n",
    "                comment_texts = [comment[\"body\"] for comment in post.get(\"comments\", []) if comment.get(\"body\")]\n",
    "                comment_scores = [get_sentiment_score(text) for text in comment_texts]\n",
    "                avg_comment_sentiment = np.mean(comment_scores) if comment_scores else 0.0\n",
    "                \n",
    "                # Perform advanced linguistic analysis on the narrative.\n",
    "                linguistic_features = advanced_linguistic_analysis(narrative_text)\n",
    "                \n",
    "                narrative_info = {\n",
    "                    \"subreddit\": subreddit,\n",
    "                    \"post_id\": post.get(\"id\"),\n",
    "                    \"title\": post.get(\"title\"),\n",
    "                    \"selftext\": post.get(\"selftext\"),\n",
    "                    \"sentiment_score\": sentiment_score,\n",
    "                    \"avg_comment_sentiment\": avg_comment_sentiment,\n",
    "                    \"score\": post.get(\"score\"),\n",
    "                    \"num_comments\": post.get(\"num_comments\"),\n",
    "                    \"url\": post.get(\"url\"),\n",
    "                    \"linguistic_features\": linguistic_features\n",
    "                }\n",
    "                emotional_narratives.append(narrative_info)\n",
    "    logging.info(\"Extracted %d emotional narratives.\", len(emotional_narratives))\n",
    "    return emotional_narratives\n",
    "\n",
    "emotional_narratives = extract_emotional_narratives(reddit_data)\n",
    "print(\"\\nExtracted Emotional Narratives (showing linguistic features for each):\")\n",
    "for narrative in emotional_narratives:\n",
    "    print(f\"Subreddit: {narrative['subreddit']}, Post ID: {narrative['post_id']}, Sentiment: {narrative['sentiment_score']:.2f}\")\n",
    "    # Optionally, print the advanced linguistic analysis results:\n",
    "    # print(narrative['linguistic_features'])\n",
    "\n",
    "# Detailed Data Analysis & Visualisation\n",
    "#\n",
    "# We now convert the extracted narratives to a DataFrame and create visualisations that illustrate:\n",
    "# 1. Narrative distribution by subreddit and sentiment.\n",
    "# 2. Sentiment score distribution.\n",
    "# 3. Engagement (upvotes and comments) versus sentiment.\n",
    "# 4. Emotional arc progression for a sample narrative.\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Convert narratives to a DataFrame\n",
    "df_narratives = pd.DataFrame(emotional_narratives)\n",
    "\n",
    "if df_narratives.empty:\n",
    "    logging.warning(\"No emotional narratives available for visualisation.\")\n",
    "else:\n",
    "    # Define sentiment categories\n",
    "    def categorize_sentiment(score):\n",
    "        if score > 0.2:\n",
    "            return \"Positive\"\n",
    "        elif score < -0.2:\n",
    "            return \"Negative\"\n",
    "        else:\n",
    "            return \"Neutral\"\n",
    "    df_narratives[\"sentiment_category\"] = df_narratives[\"sentiment_score\"].apply(categorize_sentiment)\n",
    "\n",
    "    # 1. Bar Chart: Narrative Distribution by Subreddit and Sentiment\n",
    "    fig_bar = px.bar(\n",
    "        df_narratives,\n",
    "        x=\"subreddit\",\n",
    "        color=\"sentiment_category\",\n",
    "        title=\"Narrative Count by Subreddit & Sentiment\",\n",
    "        labels={\"subreddit\": \"Subreddit\", \"count\": \"Number of Narratives\"},\n",
    "        barmode=\"group\"\n",
    "    )\n",
    "    fig_bar.update_layout(xaxis_title=\"Subreddit\", yaxis_title=\"Count\")\n",
    "    fig_bar.show()\n",
    "\n",
    "    # 2. Histogram: Sentiment Score Distribution\n",
    "    fig_hist = px.histogram(\n",
    "        df_narratives,\n",
    "        x=\"sentiment_score\",\n",
    "        nbins=20,\n",
    "        title=\"Distribution of Compound Sentiment Scores\",\n",
    "        labels={\"sentiment_score\": \"Compound Sentiment Score\", \"count\": \"Frequency\"}\n",
    "    )\n",
    "    fig_hist.update_layout(xaxis_title=\"Compound Sentiment Score\", yaxis_title=\"Frequency\")\n",
    "    fig_hist.show()\n",
    "\n",
    "    # 3. Scatter Plot: Engagement vs. Sentiment\n",
    "    fig_scatter = px.scatter(\n",
    "        df_narratives,\n",
    "        x=\"sentiment_score\",\n",
    "        y=\"score\",\n",
    "        size=\"num_comments\",\n",
    "        color=\"subreddit\",\n",
    "        title=\"Engagement vs. Sentiment\",\n",
    "        labels={\"sentiment_score\": \"Compound Sentiment Score\", \"score\": \"Upvotes\", \"num_comments\": \"Comments\"},\n",
    "        hover_data=[\"title\"]\n",
    "    )\n",
    "    fig_scatter.update_layout(xaxis_title=\"Compound Sentiment Score\", yaxis_title=\"Upvotes\")\n",
    "    fig_scatter.show()\n",
    "\n",
    "    # 4. Emotional Arc Plot: Analyse a sample narrative\n",
    "    sample_narrative = df_narratives.iloc[0]\n",
    "    narrative_text = f\"{sample_narrative['title']}\\n{sample_narrative['selftext']}\"\n",
    "    doc = nlp(narrative_text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    sentence_sentiments = [get_sentiment_score(sentence) for sentence in sentences]\n",
    "    \n",
    "    df_arc = pd.DataFrame({\n",
    "        \"Sentence\": sentences,\n",
    "        \"Sentiment Score\": sentence_sentiments,\n",
    "        \"Position\": range(1, len(sentences) + 1)\n",
    "    })\n",
    "    \n",
    "    fig_arc = px.line(\n",
    "        df_arc,\n",
    "        x=\"Position\",\n",
    "        y=\"Sentiment Score\",\n",
    "        markers=True,\n",
    "        title=\"Emotional Arc of a Sample Narrative\",\n",
    "        labels={\"Position\": \"Sentence Number\", \"Sentiment Score\": \"Compound Sentiment Score\"}\n",
    "    )\n",
    "    fig_arc.update_layout(xaxis_title=\"Sentence Number\", yaxis_title=\"Compound Sentiment Score\")\n",
    "    fig_arc.show()\n",
    "\n",
    "# Advanced Narrative Processing, Influencer Mapping & Interactive Dashboard\n",
    "#\n",
    "# In this final section, we:\n",
    "# - Cluster narratives using TF-IDF and KMeans.\n",
    "# - Extract key entities using spaCy’s NER.\n",
    "# - Build an entity co-occurrence network (a proxy for influencer mapping).\n",
    "# - Create an interactive dashboard (using ipywidgets) to filter and explore narratives.\n",
    "#\n",
    "# These advanced features further refine the insights for targeted marketing.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a combined narrative text for clustering and further analysis.\n",
    "df_narratives[\"narrative_text\"] = df_narratives[\"title\"].fillna('') + \" \" + df_narratives[\"selftext\"].fillna('')\n",
    "\n",
    "# TF-IDF vectorisation and KMeans clustering\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "X = vectorizer.fit_transform(df_narratives[\"narrative_text\"])\n",
    "num_clusters = 3\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "df_narratives[\"cluster\"] = kmeans.fit_predict(X)\n",
    "print(\"\\nCluster Distribution:\")\n",
    "print(df_narratives[\"cluster\"].value_counts())\n",
    "\n",
    "# %% [code]\n",
    "def extract_entities(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract key entities (PERSON, ORG, GPE, EVENT) from the text using spaCy.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'GPE', 'EVENT']]\n",
    "\n",
    "df_narratives[\"entities\"] = df_narratives[\"narrative_text\"].apply(extract_entities)\n",
    "print(\"\\nSample Entity Extraction:\")\n",
    "print(df_narratives[[\"title\", \"entities\"]].head())\n",
    "\n",
    "# %% [code]\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "# Build an entity co-occurrence network\n",
    "G = nx.Graph()\n",
    "for idx, row in df_narratives.iterrows():\n",
    "    entities = row[\"entities\"]\n",
    "    unique_entities = list(set(entities))\n",
    "    for ent in unique_entities:\n",
    "        if not G.has_node(ent):\n",
    "            G.add_node(ent)\n",
    "    for ent1, ent2 in combinations(unique_entities, 2):\n",
    "        if G.has_edge(ent1, ent2):\n",
    "            G[ent1][ent2]['weight'] += 1\n",
    "        else:\n",
    "            G.add_edge(ent1, ent2, weight=1)\n",
    "print(\"\\nEntity Network: Nodes =\", G.number_of_nodes(), \", Edges =\", G.number_of_edges())\n",
    "\n",
    "# Visualise the entity network using Plotly\n",
    "pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "edge_x, edge_y = [], []\n",
    "for edge in G.edges():\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    edge_x.extend([x0, x1, None])\n",
    "    edge_y.extend([y0, y1, None])\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x, y=edge_y,\n",
    "    line=dict(width=0.5, color='#888'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines'\n",
    ")\n",
    "node_x, node_y, node_text = [], [], []\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    node_text.append(node)\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers+text',\n",
    "    text=node_text,\n",
    "    textposition=\"top center\",\n",
    "    hoverinfo='text',\n",
    "    marker=dict(\n",
    "        showscale=True,\n",
    "        colorscale='YlGnBu',\n",
    "        reversescale=True,\n",
    "        color=[],\n",
    "        size=10,\n",
    "        colorbar=dict(\n",
    "            thickness=15,\n",
    "            title='Node Connections',\n",
    "            xanchor='left',\n",
    "            titleside='right'\n",
    "        ),\n",
    "        line_width=2)\n",
    ")\n",
    "node_adjacencies = [len(list(G.neighbours(node))) for node in G.nodes()]\n",
    "node_trace.marker.color = node_adjacencies\n",
    "\n",
    "fig_network = go.Figure(data=[edge_trace, node_trace],\n",
    "             layout=go.Layout(\n",
    "                title='Entity Co-occurrence Network',\n",
    "                titlefont_size=16,\n",
    "                showlegend=False,\n",
    "                hovermode='closest',\n",
    "                margin=dict(b=20, l=5, r=5, t=40),\n",
    "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
    "            ))\n",
    "fig_network.show()\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Create widgets for the interactive dashboard\n",
    "cluster_options = sorted(df_narratives[\"cluster\"].unique())\n",
    "cluster_dropdown = widgets.Dropdown(\n",
    "    options=[(\"All\", \"all\")] + [(f\"Cluster {i}\", i) for i in cluster_options],\n",
    "    value=\"all\",\n",
    "    description=\"Cluster:\"\n",
    ")\n",
    "all_entities = list(set([ent for sublist in df_narratives[\"entities\"] for ent in sublist]))\n",
    "entity_select = widgets.SelectMultiple(\n",
    "    options=sorted(all_entities),\n",
    "    description='Entities:',\n",
    "    disabled=False\n",
    ")\n",
    "output = widgets.Output()\n",
    "\n",
    "def update_dashboard(change):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        if cluster_dropdown.value == \"all\":\n",
    "            filtered_df = df_narratives.copy()\n",
    "        else:\n",
    "            filtered_df = df_narratives[df_narratives[\"cluster\"] == cluster_dropdown.value]\n",
    "        selected_entities = list(entity_select.value)\n",
    "        if selected_entities:\n",
    "            filtered_df = filtered_df[filtered_df[\"entities\"].apply(lambda ents: any(e in ents for e in selected_entities))]\n",
    "        display(filtered_df[[\"subreddit\", \"title\", \"sentiment_score\", \"cluster\", \"entities\"]])\n",
    "\n",
    "cluster_dropdown.observe(update_dashboard, names='value')\n",
    "entity_select.observe(update_dashboard, names='value')\n",
    "\n",
    "dashboard = widgets.VBox([cluster_dropdown, entity_select, output])\n",
    "display(dashboard)\n",
    "update_dashboard(None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
